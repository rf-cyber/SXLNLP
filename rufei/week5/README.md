# 知识点总结
## 词向量
### word embedding 和 word vector
二者都是表示将词转换为词向量的这个事
具体区别可以：word embedding指对词向量进行初始化
word2vector是对初始化的word embedding进行训练

### 基于语言模型
词向量是实现这种目标下的副产品
因为模型中有其他层的存在，模型的效果还会有其他层的效果

### 基于窗口
词向量是在实现这种目标下的副产品
#### 1. CBOW

- 上下文预测目标词

#### 2. Skip-gram

- 目标词预测上下文

#### 3. huffman树：
- 替代方案：历史上减少运算量的工程方法，现在可以不用了
- 建立词频的huffman树，将词频高的词放在树的根部，词频低的词放在树的叶子

#### 4. 负采样 negative sampling
正样本保留，负样本随机选。
- 负样本好找，正样本不好找
- 负采样：将词频高的词作为正样本，将词频低的词作为负样本
- 替代方案：历史上减少运算量的工程方法，现在可以不用了

### 基于共现矩阵 Glove
共现矩阵：V * V
- 共现矩阵：词与词之间的共现关系
- 矩阵中的值：每个词共同出现的次数
- 我们可以计算共现矩阵的共现概率。

## 词向量训练总结
- 根据词语词之间关系的某种规律，指定训练目标
- 设计模型，以词向量为输入
- 词向量的训练都是从随机初始化开始训练
- 训练过程中，词向量作为参数不断调整，获取一定的语义信息
- 使用训练好的词向量做下游任务

## 词向量存在的问题
- 词向量是“静态”的。每个词使用固定向量，没有考虑前后文。这种事不完善的。
- 一词多义的情况在训练的时候容易混淆。西瓜 - 华为  - 华为。向量近似，容易混淆
- 影响效果的因素很多
  - 维度选择
  - 随机初始化
  - skip-gram
  - cbow
  - glove
  - 分词质量
  - 词频截断
  - 窗口大小
  - 迭代次数
  - 停止条件
  - 预料质量等
- 没有好的直接评价指标。常需要用下游任务来评价。所以现在都是建议在模型中随机初始化后，参与模型训练。（以前很鼓励单独训练词向量，现在不鼓励）

## 词向量应用
### 句子应用
- 将一句话或者一段文本分成若干个词
- 找到

### K-Means
- 随机训责k个点作为初始质心
- repeat
  - 计算每个点与这k个质心的距离
  - 将每个点指派到最近的质心，形成k个簇
  - 重新计算每个簇的质心
- until
  - 质心不发生变化或者质心变化小于阈值
- 优点
  - 速度很快，可以支持很大量的数据
  - 样本均匀特征明显的情况下，效果不错
- 缺点
  - 认为设定聚类数量
  - 初始化中心影响效果，导致结果不稳定
  - 对于个别特殊样本敏感，会大幅影响聚类中心位置。
  - 不适合多标签(模棱两可的样本)或者样本较为离散的数据
- 使用技巧
  - 先设定较多的聚类类别
  - 聚类结束后计算类内平均距离(与质心的距离)
  - 排序后，舍弃类内平均距离较长的类别
  - 计算距离：欧氏距离，余弦距离，或者其他距离
  - 短文本聚类：记得先去重，以及其他预处理。

词向量总结：
1. 质变：将离散的字符转化为连续的数值
2. 通过向量的相似度代表语义的相似度
3. 词向量的训练基于很多不完全正确的假设，但是据此训练的词向量是有意义的
4. 使用五标注的文本的一种好方法

词向量现在更多的被主流方法代替：预训练模型。

# 作业要求

基于词向量Kmeans使用代码，结合kmeans使用技巧，计算平均距离，舍弃类内平均距离较长的类别。

# 一、需求分析

根据作业要求，确定需求实现目标，提取需求约束条件，拆分成子模块进行实现。

## US_20240713_05_001_1

- 需求背景
- 需求描述
- 约束条件

## 数据标注


# 二、需求设计

## US_20240713_05_001_1

### 背景

### 实现方式

### 结果评估

# 三、需求实现

## 需求1

### byTorch

Model 1实现

### byMySelf

Model 1实现

## Requirement 2

### byTorch

Model 2实现

### byMySelf

Model 2实现

# 四、模型测试

## Model 1

### 测试用例

### 测试结果

### 结果评估

## Model 2

### 测试用例

### 测试结果

### 结果评估

# 五、模型部署与维护

## 部署方式

## 维护方式

# 六、模型优化思考

